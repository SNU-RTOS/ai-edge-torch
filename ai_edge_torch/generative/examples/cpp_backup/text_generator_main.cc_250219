#include <algorithm>
#include <cstddef>
#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <cmath>
#include <fstream>
#include <ios>
#include <iterator>
#include <limits>
#include <map>
#include <memory>
#include <string>
#include <vector>
#include <chrono>
#include <random>
#include <iostream> // std::cout, std::endl
#include <sys/mman.h>
//  #include <errno.h>

#include "absl/flags/flag.h"
#include "absl/flags/parse.h"
#include "absl/strings/match.h"
#include "ai_edge_torch/generative/examples/cpp/utils.h"
#include "src/sentencepiece_processor.h"
#include "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h"
#include "tensorflow/lite/experimental/genai/genai_ops.h"
#include "tensorflow/lite/interpreter.h"
#include "tensorflow/lite/interpreter_builder.h"
#include "tensorflow/lite/kernels/register.h"
#include "tensorflow/lite/model_builder.h"
#include "tensorflow/lite/signature_runner.h"
// #ifdef TF_LITE_TENSORFLOW_PROFILER
// #include "tensorflow/lite/profiling/profiler.h"
// #endif

// ----------------------
// absl::FLAGS definition
// ----------------------
ABSL_FLAG(std::string, tflite_model, "",
          "Two-signature tflite model for text generation using ODML tools.");
ABSL_FLAG(std::string, sentencepiece_model, "", "Path to the SentencePiece model file.");
ABSL_FLAG(std::string, prompt, "Write an email:", "Input prompt for the model.");
ABSL_FLAG(int, max_decode_steps, -1,
          "Number of tokens to generate. Defaults to the KV cache limit.");
ABSL_FLAG(std::string, start_token, "",
          "Optional start token appended to the beginning of the input prompt.");
ABSL_FLAG(std::string, stop_token, "",
          "Optional stop token that stops the decoding loop if encountered.");
ABSL_FLAG(int, num_threads, 4, "Number of threads to use. Defaults to 4.");
ABSL_FLAG(std::string, weight_cache_path, "",
          "Path for XNNPACK weight caching, e.g., /tmp/model.xnnpack_cache.");
ABSL_FLAG(std::string, lora_path, "", "Optional path to a LoRA artifact.");

namespace
{

    using ai_edge_torch::examples::AlignedAllocator;
    using ai_edge_torch::examples::LoRA;

    // --------------------------------------------------------------------------
    // A scoped timer that prints the elapsed time when going out of scope
    // --------------------------------------------------------------------------
    class ScopeTimer
    {
    public:
        explicit ScopeTimer(const std::string &name)
            : name_(name),
              start_(std::chrono::high_resolution_clock::now()) {}

        ~ScopeTimer()
        {
            auto end = std::chrono::high_resolution_clock::now();
            auto duration_ms =
                std::chrono::duration_cast<std::chrono::milliseconds>(end - start_).count();
            std::cout << "\n[INFO] " << name_ << " took " << duration_ms << " ms\n";
        }

    private:
        std::string name_;
        std::chrono::high_resolution_clock::time_point start_;
    };

    // --------------------------------------------------------------------------
    // Class for measuring decoding metrics (time to first token, average times, etc.)
    // --------------------------------------------------------------------------
    class DecodingMetrics
    {
    public:
        // Called before decoding loop starts
        void StartDecoding()
        {
            decode_start_ = std::chrono::high_resolution_clock::now();
        }

        // Record times for each token
        //   - token_start: time point before inference/sampling starts for a token
        //   - inference_time_ms: how many ms were spent in model inference
        //   - sampling_time_ms : how many ms were spent in sampling the next token
        void RecordTimes(const std::chrono::high_resolution_clock::time_point &token_start,
                         double inference_time_ms, double sampling_time_ms)
        {
            auto token_end = std::chrono::high_resolution_clock::now();
            double decoding_time_ms =
                std::chrono::duration<double, std::milli>(token_end - token_start).count();

            // If this is the first token, record time to first token
            if (!first_token_recorded_)
            {
                first_token_recorded_ = true;
                time_to_first_token_ms_ =
                    std::chrono::duration<double, std::milli>(token_end - decode_start_).count();
            }

            // Track inference time
            total_inference_time_ms_ += inference_time_ms;
            // Track sampling time
            total_sampling_time_ms_ += sampling_time_ms;
            // Track total decoding time
            total_decoding_time_ms_ += decoding_time_ms;

            // Track total tokens
            ++token_count_;
        }

        // Print out final decoding metrics
        void PrintMetrics() const
        {
            double avg_inference_time_ms = 0.0;
            double avg_sampling_time_ms = 0.0;
            double avg_decoding_time_ms = 0.0;
            double avg_inference_speed = 0.0;
            double avg_sampling_speed = 0.0;
            double avg_decoding_speed = 0.0;

            if (token_count_ > 0)
            {
                avg_inference_time_ms = total_inference_time_ms_ / token_count_;
                avg_sampling_time_ms = total_sampling_time_ms_ / token_count_;
                avg_decoding_time_ms = (total_sampling_time_ms_ + total_inference_time_ms_) / token_count_;

                avg_inference_speed = token_count_ / (total_inference_time_ms_ / 1000);
                avg_sampling_speed = token_count_ / (total_sampling_time_ms_ / 1000);
                avg_decoding_speed = token_count_ / (total_decoding_time_ms_ / 1000);
            }

            std::cout << "\n\n================================\n";
            std::cout << "[INFO] Decoding stage completed\n";
            std::cout << "[METRICS] Total Number of Generated Tokens : " << token_count_ << " tokens\n\n";

            std::cout << "[METRICS] Total Inference Latency          : " << total_inference_time_ms_ << " ms\n";
            std::cout << "[METRICS] Total Sampling Latency           : " << total_sampling_time_ms_ << " ms\n";
            std::cout << "[METRICS] Total Decoding Latency           : " << total_decoding_time_ms_ << " ms\n\n";

            std::cout << "[METRICS] Time To First Token              : " << time_to_first_token_ms_ << " ms\n";
            std::cout << "[METRICS] Average Inference Latency        : " << avg_inference_time_ms << " ms/tokens"
                      << "(" << avg_inference_speed << " token/s )\n";
            std::cout << "[METRICS] Average Sampling Latency         : " << avg_sampling_time_ms << " ms/tokens"
                      << "(" << avg_sampling_speed << " token/s )\n";
            std::cout << "[METRICS] Average Decoding Latency         : " << avg_decoding_time_ms << " ms/tokens"
                      << "(" << avg_decoding_speed << " token/s )\n";
        }

    private:
        // Decode start time
        std::chrono::high_resolution_clock::time_point decode_start_;

        // Time to first token
        double time_to_first_token_ms_ = 0.0;
        bool first_token_recorded_ = false;

        // Accumulators
        double total_inference_time_ms_ = 0.0;
        double total_sampling_time_ms_ = 0.0;
        double total_decoding_time_ms_ = 0.0;
        int token_count_ = 0;
    };

    ////// Memory Reordering
    std::unique_ptr<uint8_t[]> g_combined_buffer = nullptr;
    size_t g_buffer_size = 0;

    // Function to free the global buffer if needed
    void freeReorganizedTensorBuffer() {
        if (g_combined_buffer) {
            g_combined_buffer.reset();
            g_buffer_size = 0;
            // std::cout << "Freed reorganized tensor buffer\n";
        }
    }

    void reorderTensorV2(std::unique_ptr<tflite::Interpreter>& interpreter) {
        constexpr size_t BLOCK_SIZE = 4096;
        
        // Free any existing buffer before reorganizing
        freeReorganizedTensorBuffer();
        
        const std::vector<int>& execution_plan = interpreter->execution_plan();
        
        // Enhanced tensor info structure
        struct TensorInfo {
            size_t first_occurrence;  // First node that uses this tensor
            size_t last_occurrence;   // Last node that uses this tensor
            size_t size;             // Tensor size in bytes
            std::vector<size_t> usage_nodes;  // All nodes that use this tensor
        };
        
        std::unordered_map<int, TensorInfo> tensor_info;
        size_t total_size = 0;
        
        // First pass: Find all occurrences of memory-mapped tensors
        for (size_t i = 0; i < execution_plan.size(); ++i) {
            const auto* node = interpreter->node_and_registration(execution_plan[i]);
            const auto* node_inputs = node->first.inputs;
            
            for (int j = 0; j < node_inputs->size; ++j) {
                int tensor_index = node_inputs->data[j];
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                
                if (tensor->allocation_type != kTfLiteMmapRo) {
                    continue;
                }
                
                auto& info = tensor_info[tensor_index];
                if (info.usage_nodes.empty()) {
                    info.first_occurrence = i;
                    info.size = tensor->bytes;
                    total_size += tensor->bytes;
                }
                info.last_occurrence = i;
                info.usage_nodes.push_back(i);
            }
        }
        
        if (total_size == 0) {
            return;
        }
    
        // Allocate global combined buffer with extra space for alignment
        g_combined_buffer = std::make_unique<uint8_t[]>(total_size + BLOCK_SIZE);
        g_buffer_size = total_size + BLOCK_SIZE;
        
        // Separate tensors into large (>= BLOCK_SIZE) and small (< BLOCK_SIZE) tensors
        std::vector<std::pair<int, TensorInfo>> large_tensors;
        std::vector<std::pair<int, TensorInfo>> small_tensors;
        
        for (const auto& [tensor_index, info] : tensor_info) {
            if (info.size >= BLOCK_SIZE) {
                large_tensors.push_back({tensor_index, info});
            } else {
                small_tensors.push_back({tensor_index, info});
            }
        }
        
        // Sort large tensors by first occurrence
        std::sort(large_tensors.begin(), large_tensors.end(),
            [](const auto& a, const auto& b) {
                return a.second.first_occurrence < b.second.first_occurrence;
            });
        
        // Sort small tensors by first occurrence, then by size (larger first)
        std::sort(small_tensors.begin(), small_tensors.end(),
            [](const auto& a, const auto& b) {
                if (a.second.first_occurrence == b.second.first_occurrence) {
                    return a.second.size > b.second.size;
                }
                return a.second.first_occurrence < b.second.first_occurrence;
            });
        
        // Group small tensors into blocks
        struct BlockGroup {
            std::vector<int> tensor_indices;
            size_t total_size = 0;
            size_t first_occurrence = SIZE_MAX;
            size_t last_occurrence = 0;
        };
        
        std::vector<BlockGroup> block_groups;
        
        // Group small tensors
        for (const auto& [tensor_index, info] : small_tensors) {
            bool added_to_existing = false;
            
            // Try to add to an existing group if there's temporal locality
            for (auto& group : block_groups) {
                // Check if this tensor is temporally close to the group
                bool is_temporally_close = 
                    (info.first_occurrence <= group.last_occurrence + 3) &&
                    (info.last_occurrence >= group.first_occurrence - 3);
                
                // Check if it fits in the current block
                bool fits_in_block = (group.total_size + info.size <= BLOCK_SIZE);
                
                if (is_temporally_close && fits_in_block) {
                    group.tensor_indices.push_back(tensor_index);
                    // if(info.size == 0){
                    //     group.total_size += 16;
                    // } else {
                    //     group.total_size += info.size;
                    // }
                    group.total_size += info.size;
                    group.first_occurrence = std::min(group.first_occurrence, info.first_occurrence);
                    group.last_occurrence = std::max(group.last_occurrence, info.last_occurrence);
                    added_to_existing = true;
                    break;
                }
            }
            
            // If didn't fit in any existing group, create a new one
            if (!added_to_existing) {
                BlockGroup new_group;
                new_group.tensor_indices.push_back(tensor_index);
                // if(info.size == 0){
                //     new_group.total_size = 16;
                // } else {
                //     new_group.total_size = info.size;
                // }
                new_group.total_size = info.size;
                new_group.first_occurrence = info.first_occurrence;
                new_group.last_occurrence = info.last_occurrence;
                block_groups.push_back(new_group);
            }
        }
        
        // Sort block groups by first occurrence
        std::sort(block_groups.begin(), block_groups.end(),
            [](const auto& a, const auto& b) {
                return a.first_occurrence < b.first_occurrence;
            });
        
        // Copy data and update tensor pointers, handling both large and small tensors
        size_t current_offset = 0;
        
        // Process large tensors first, aligning each to block boundary
        for (const auto& [tensor_index, info] : large_tensors) {
            // Align to block boundary
            size_t aligned_offset = (current_offset + BLOCK_SIZE - 1) & ~(BLOCK_SIZE - 1);
            current_offset = aligned_offset;
            
            TfLiteTensor* tensor = interpreter->tensor(tensor_index);
            
            // Copy data to new location
            std::memcpy(g_combined_buffer.get() + current_offset, 
                       tensor->data.raw, 
                       tensor->bytes);
            
            // Update tensor pointer
            tensor->data.raw = reinterpret_cast<char*>(g_combined_buffer.get() + current_offset);
            
            // if(tensor->bytes == 0) {
            //     current_offset += 16;
            // } else{
            //     current_offset += tensor->bytes;
            // }
            current_offset += tensor->bytes;
        }
        
        // Process block groups (small tensors)
        for (const auto& group : block_groups) {
            // Align to block boundary
            size_t aligned_offset = (current_offset + BLOCK_SIZE - 1) & ~(BLOCK_SIZE - 1);
            current_offset = aligned_offset;
            
            // Copy all tensors in the group
            for (int tensor_index : group.tensor_indices) {
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                
                // Copy data to new location
                std::memcpy(g_combined_buffer.get() + current_offset, 
                           tensor->data.raw, 
                           tensor->bytes);
                
                // Update tensor pointer
                tensor->data.raw = reinterpret_cast<char*>(g_combined_buffer.get() + current_offset);
                
                // if(tensor->bytes == 0) {
                //     current_offset += 16;
                // } else{
                //     current_offset += tensor->bytes;
                // }
                current_offset += tensor->bytes;
            }
        }
        
        // Update final buffer size
        g_buffer_size = current_offset;
    }

    // reorder Hot/Warm/Cold
    void reorderTensorV3(std::unique_ptr<tflite::Interpreter>& interpreter) {
        constexpr size_t BLOCK_SIZE = 4096;
        constexpr size_t CACHE_LINE_SIZE = 64;  // Common cache line size
        
        freeReorganizedTensorBuffer();
        
        const std::vector<int>& execution_plan = interpreter->execution_plan();
        
        struct TensorInfo {
            size_t first_occurrence;
            size_t last_occurrence;
            size_t size;
            std::vector<size_t> usage_nodes;
            float access_frequency;      // How often the tensor is accessed
            bool is_activation;          // Whether this tensor is an activation
            std::vector<size_t> concurrent_tensors;  // Tensors accessed together
        };
        
        std::unordered_map<int, TensorInfo> tensor_info;
        size_t total_size = 0;
        
        // First pass: Enhanced tensor usage analysis
        for (size_t i = 0; i < execution_plan.size(); ++i) {
            const auto* node = interpreter->node_and_registration(execution_plan[i]);
            const auto* node_inputs = node->first.inputs;
            const auto* node_outputs = node->first.outputs;
            
            // Track concurrent tensor access patterns
            std::vector<int> node_tensors;
            
            // Analyze inputs
            for (int j = 0; j < node_inputs->size; ++j) {
                int tensor_index = node_inputs->data[j];
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                
                if (tensor->allocation_type != kTfLiteMmapRo) {
                    continue;
                }
                
                auto& info = tensor_info[tensor_index];
                if (info.usage_nodes.empty()) {
                    info.first_occurrence = i;
                    info.size = tensor->bytes;
                    info.access_frequency = 1.0f;
                    info.is_activation = false;
                    total_size += tensor->bytes;
                } else {
                    info.access_frequency += 1.0f;
                }
                info.last_occurrence = i;
                info.usage_nodes.push_back(i);
                node_tensors.push_back(tensor_index);
            }
            
            // Analyze outputs (activations)
            for (int j = 0; j < node_outputs->size; ++j) {
                int tensor_index = node_outputs->data[j];
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                
                if (tensor->allocation_type != kTfLiteMmapRo) {
                    continue;
                }
                
                auto& info = tensor_info[tensor_index];
                info.is_activation = true;
                node_tensors.push_back(tensor_index);
            }
            
            // Update concurrent tensor information
            for (int tensor_index : node_tensors) {
                auto& info = tensor_info[tensor_index];
                info.concurrent_tensors.insert(
                    info.concurrent_tensors.end(),
                    node_tensors.begin(),
                    node_tensors.end()
                );
            }
        }
        
        // Normalize access frequencies and remove duplicates from concurrent tensors
        for (auto& [tensor_index, info] : tensor_info) {
            info.access_frequency /= execution_plan.size();
            std::sort(info.concurrent_tensors.begin(), info.concurrent_tensors.end());
            info.concurrent_tensors.erase(
                std::unique(info.concurrent_tensors.begin(), info.concurrent_tensors.end()),
                info.concurrent_tensors.end()
            );
        }
        
        // Allocate combined buffer with extra space for alignment
        g_combined_buffer = std::make_unique<uint8_t[]>(total_size + BLOCK_SIZE);
        g_buffer_size = total_size + BLOCK_SIZE;
        
        // Enhanced tensor classification
        struct TensorGroup {
            std::vector<int> tensor_indices;
            size_t total_size = 0;
            float priority_score = 0.0f;
        };
        
        // Separate tensors into different categories
        std::vector<TensorGroup> hot_groups;      // Frequently accessed tensors
        std::vector<TensorGroup> warm_groups;     // Moderately accessed tensors
        std::vector<TensorGroup> cold_groups;     // Rarely accessed tensors
        
        // Helper function to calculate tensor priority
        auto calculate_priority = [&](const TensorInfo& info) {
            float temporal_score = 1.0f / (info.last_occurrence - info.first_occurrence + 1);
            float size_score = 1.0f / (info.size + 1);
            return info.access_frequency * temporal_score + size_score;
        };
        
        // Group tensors based on access patterns and size
        for (const auto& [tensor_index, info] : tensor_info) {
            float priority = calculate_priority(info);
            
            TensorGroup new_group;
            new_group.tensor_indices.push_back(tensor_index);
            new_group.total_size = info.size;
            new_group.priority_score = priority;
            
            if (priority > 0.7f) {
                hot_groups.push_back(new_group);
            } else if (priority > 0.3f) {
                warm_groups.push_back(new_group);
            } else {
                cold_groups.push_back(new_group);
            }
        }
        
        // Sort groups by priority score
        auto sort_groups = [](std::vector<TensorGroup>& groups) {
            std::sort(groups.begin(), groups.end(),
                [](const auto& a, const auto& b) {
                    return a.priority_score > b.priority_score;
                });
        };
        
        sort_groups(hot_groups);
        sort_groups(warm_groups);
        sort_groups(cold_groups);
        
        // Memory layout optimization
        size_t current_offset = 0;
        
        // Helper function to align offset
        auto align_offset = [](size_t offset, size_t alignment) {
            return (offset + alignment - 1) & ~(alignment - 1);
        };
        
        // Process hot tensors (cache-line aligned)
        for (const auto& group : hot_groups) {
            current_offset = align_offset(current_offset, CACHE_LINE_SIZE);
            for (int tensor_index : group.tensor_indices) {
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                std::memcpy(g_combined_buffer.get() + current_offset,
                           tensor->data.raw,
                           tensor->bytes);
                tensor->data.raw = reinterpret_cast<char*>(g_combined_buffer.get() + current_offset);
                current_offset += tensor->bytes;
            }
        }
        
        // Process warm tensors (block aligned)
        for (const auto& group : warm_groups) {
            current_offset = align_offset(current_offset, BLOCK_SIZE);
            for (int tensor_index : group.tensor_indices) {
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                std::memcpy(g_combined_buffer.get() + current_offset,
                           tensor->data.raw,
                           tensor->bytes);
                tensor->data.raw = reinterpret_cast<char*>(g_combined_buffer.get() + current_offset);
                current_offset += tensor->bytes;
            }
        }
        
        // Process cold tensors (minimal alignment requirements)
        for (const auto& group : cold_groups) {
            current_offset = align_offset(current_offset, sizeof(void*));
            for (int tensor_index : group.tensor_indices) {
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                std::memcpy(g_combined_buffer.get() + current_offset,
                           tensor->data.raw,
                           tensor->bytes);
                tensor->data.raw = reinterpret_cast<char*>(g_combined_buffer.get() + current_offset);
                current_offset += tensor->bytes;
            }
        }
        
        g_buffer_size = current_offset;
    }

    // ReorderTensor Version4
    void reorderTensorV4(std::unique_ptr<tflite::Interpreter>& interpreter) {
        if (!interpreter) {
            return;
        }
    
        constexpr size_t BLOCK_SIZE = 4096;
        
        // Free any existing buffer before reorganizing
        freeReorganizedTensorBuffer();
        
        const std::vector<int>& execution_plan = interpreter->execution_plan();
        if (execution_plan.empty()) {
            return;
        }
        
        struct TensorInfo {
            size_t first_step;
            size_t size;
            const void* data;
        };
        
        std::unordered_map<int, TensorInfo> tensor_info;
        size_t total_size = 0;
        
        // First pass: Find first occurrence of each tensor in execution plan
        for (size_t step = 0; step < execution_plan.size(); ++step) {
            int node_index = execution_plan[step];
            const auto* node_and_reg = interpreter->node_and_registration(node_index);
            if (!node_and_reg) {
                continue;
            }
    
            const TfLiteNode& node = node_and_reg->first;
            if (!node.inputs || !node.inputs->data) {
                continue;
            }
            
            // Check inputs for memory-mapped tensors
            for (int j = 0; j < node.inputs->size; ++j) {
                int tensor_index = node.inputs->data[j];
                if (tensor_index < 0) {
                    continue;
                }
    
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                if (!tensor || !tensor->data.raw) {
                    continue;
                }
                
                if (tensor->allocation_type != kTfLiteMmapRo) {
                    continue;
                }
                
                auto it = tensor_info.find(tensor_index);
                if (it == tensor_info.end()) {
                    // First occurrence of this tensor
                    TensorInfo info;
                    info.first_step = step;
                    info.size = tensor->bytes;
                    info.data = tensor->data.raw;
                    tensor_info[tensor_index] = info;
                    total_size += tensor->bytes;
                }
            }
        }
        
        if (total_size == 0) {
            return;
        }
        
        try {
            // Allocate global combined buffer with extra space for alignment
            g_combined_buffer = std::make_unique<uint8_t[]>(total_size + BLOCK_SIZE);
            g_buffer_size = total_size + BLOCK_SIZE;
        } catch (const std::bad_alloc&) {
            return;  // Handle allocation failure gracefully
        }
        
        // Group tensors by execution step
        std::map<size_t, std::vector<std::pair<int, const TensorInfo*>>> tensors_by_step;
        
        for (const auto& [tensor_index, info] : tensor_info) {
            tensors_by_step[info.first_step].push_back({tensor_index, &info});
        }
        
        // Sort tensors within each execution step by size
        for (auto& [step, tensors] : tensors_by_step) {
            std::sort(tensors.begin(), tensors.end(),
                [](const auto& a, const auto& b) {
                    return a.second->size > b.second->size;  // Larger tensors first
                });
        }
        
        // Copy tensors in execution order, maintaining size-based ordering within each step
        size_t current_offset = 0;
        
        for (size_t step = 0; step < execution_plan.size(); ++step) {
            auto it = tensors_by_step.find(step);
            if (it == tensors_by_step.end()) {
                continue;
            }
            
            for (const auto& [tensor_index, info] : it->second) {
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                if (!tensor || !tensor->data.raw) {
                    continue;
                }
                
                // Align to block boundary for better memory access
                size_t aligned_offset = (current_offset + BLOCK_SIZE - 1) & ~(BLOCK_SIZE - 1);
                if (aligned_offset + tensor->bytes > g_buffer_size) {
                    continue;  // Skip if would exceed buffer
                }
                current_offset = aligned_offset;
                
                // Copy data to new location
                std::memcpy(g_combined_buffer.get() + current_offset,
                           tensor->data.raw,
                           tensor->bytes);
                
                // Update tensor pointer
                tensor->data.raw = reinterpret_cast<char*>(g_combined_buffer.get() + current_offset);
                
                current_offset += tensor->bytes;
            }
        }
        
        // Update final buffer size
        g_buffer_size = current_offset;
    }

    // reorderTensor Version 5 using posix_memalign
    void reorderTensorV5(std::unique_ptr<tflite::Interpreter>& interpreter) {
        if (!interpreter) {
            return;
        }
    
        const std::vector<int>& execution_plan = interpreter->execution_plan();
        if (execution_plan.empty()) {
            return;
        }
    
        struct TensorInfo {
            size_t first_step;
            size_t size;
            const void* data;
        };
    
        std::unordered_map<int, TensorInfo> tensor_info;
        size_t total_size = 0;
    
        // First pass: Find first occurrence of each tensor and calculate total size
        for (size_t step = 0; step < execution_plan.size(); ++step) {
            int node_index = execution_plan[step];
            const auto* node_and_reg = interpreter->node_and_registration(node_index);
            if (!node_and_reg) {
                continue;
            }
    
            const TfLiteNode& node = node_and_reg->first;
            if (!node.inputs || !node.inputs->data) {
                continue;
            }
    
            for (int j = 0; j < node.inputs->size; ++j) {
                int tensor_index = node.inputs->data[j];
                if (tensor_index < 0) {
                    continue;
                }
    
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                if (!tensor || !tensor->data.raw || tensor->allocation_type != kTfLiteMmapRo) {
                    continue;
                }
    
                auto it = tensor_info.find(tensor_index);
                if (it == tensor_info.end()) {
                    // Calculate aligned size for this tensor
                    size_t size = tensor->bytes;
                    size_t padding = tflite::kDefaultTensorAlignment - 
                                   (size % tflite::kDefaultTensorAlignment);
                    size += padding;
    
                    TensorInfo info;
                    info.first_step = step;
                    info.size = size;  // Store aligned size
                    info.data = tensor->data.raw;
                    tensor_info[tensor_index] = info;
                    total_size += size;
                }
            }
        }
    
        if (total_size == 0) {
            return;
        }
    
        // Allocate aligned memory for combined buffer
        void* aligned_buffer = nullptr;
        int ret = posix_memalign(&aligned_buffer, tflite::kDefaultTensorAlignment, total_size);
        if (ret != 0 || !aligned_buffer) {
            return;  // Handle allocation failure
        }
    
        // Create unique_ptr with custom deleter for aligned memory
        g_combined_buffer.reset(static_cast<uint8_t*>(aligned_buffer));
        g_buffer_size = total_size;
    
        // Group tensors by execution step
        std::map<size_t, std::vector<std::pair<int, const TensorInfo*>>> tensors_by_step;
        for (const auto& [tensor_index, info] : tensor_info) {
            tensors_by_step[info.first_step].push_back({tensor_index, &info});
        }
    
        // Sort tensors within each step by size
        for (auto& [step, tensors] : tensors_by_step) {
            std::sort(tensors.begin(), tensors.end(),
                [](const auto& a, const auto& b) {
                    return a.second->size > b.second->size;
                });
        }
    
        // Copy tensors in execution order
        size_t current_offset = 0;
        for (size_t step = 0; step < execution_plan.size(); ++step) {
            auto it = tensors_by_step.find(step);
            if (it == tensors_by_step.end()) {
                continue;
            }
    
            for (const auto& [tensor_index, info] : it->second) {
                TfLiteTensor* tensor = interpreter->tensor(tensor_index);
                if (!tensor || !tensor->data.raw) {
                    continue;
                }
    
                // Copy data to new aligned location
                std::memcpy(g_combined_buffer.get() + current_offset,
                           tensor->data.raw,
                           tensor->bytes);
    
                // Update tensor pointer
                tensor->data.raw = reinterpret_cast<char*>(g_combined_buffer.get() + current_offset);
                
                current_offset += info->size;  // Use pre-calculated aligned size
            }
        }
    }

    // Optional: Function to get the current buffer size
    size_t getReorganizedBufferSize() {
        return g_buffer_size;
    }

    // Optional: Function to check if buffer is allocated
    bool isBufferAllocated() {
        return g_combined_buffer != nullptr;
    }

    // --------------------------------------------------------------------------
    // A class that provides various sampling methods (Greedy, Top-K, Top-P, etc.)
    // --------------------------------------------------------------------------
    class Sampler
    {
    public:
        // ------------------------
        // Greedy Sampler
        // ------------------------
        static int GreedySampler(const TfLiteTensor *logits)
        {
            float max_value = -std::numeric_limits<float>::infinity();
            int max_index = 0;
            int vocab_size = logits->dims->data[2];

            for (int i = 0; i < vocab_size; ++i)
            {
                if (logits->data.f[i] > max_value)
                {
                    max_value = logits->data.f[i];
                    max_index = i;
                }
            }
            return max_index;
        }

        // ------------------------
        // Top-K Sampler
        // ------------------------
        static int TopKSampler(const TfLiteTensor *logits, int k)
        {
            int vocab_size = logits->dims->data[2];
            std::vector<std::pair<float, int>> sorted_logits;
            sorted_logits.reserve(vocab_size);

            for (int i = 0; i < vocab_size; ++i)
            {
                sorted_logits.emplace_back(logits->data.f[i], i);
            }

            // Partial sort to get the top k elements
            if (k < vocab_size)
            {
                std::partial_sort(sorted_logits.begin(), sorted_logits.begin() + k, sorted_logits.end(),
                                  std::greater<std::pair<float, int>>());
                sorted_logits.resize(k);
            }
            else
            {
                // If k >= vocab_size, no need to cut
                std::sort(sorted_logits.begin(), sorted_logits.end(), std::greater<std::pair<float, int>>());
            }

            // Compute normalized probabilities
            float sum_probs = 0.0f;
            for (auto &pair : sorted_logits)
            {
                sum_probs += std::exp(pair.first);
            }
            std::vector<float> probabilities;
            probabilities.reserve(sorted_logits.size());
            for (auto &pair : sorted_logits)
            {
                probabilities.push_back(std::exp(pair.first) / sum_probs);
            }

            // Multinomial sampling
            std::random_device rd;
            std::mt19937 gen(rd());
            std::discrete_distribution<> dist(probabilities.begin(), probabilities.end());

            return sorted_logits[dist(gen)].second;
        }

        // ------------------------
        // Top-P (Nucleus) Sampler
        // ------------------------
        static int TopPSampler(const TfLiteTensor *logits, float p)
        {
            int vocab_size = logits->dims->data[2];
            std::vector<std::pair<float, int>> sorted_logits;
            sorted_logits.reserve(vocab_size);

            for (int i = 0; i < vocab_size; ++i)
            {
                sorted_logits.emplace_back(logits->data.f[i], i);
            }

            // Sort descending by logit value
            std::sort(sorted_logits.begin(), sorted_logits.end(),
                      std::greater<std::pair<float, int>>());

            // Apply softmax to get probabilities
            std::vector<float> probabilities(vocab_size);
            float sum_exp = 0.0f;
            for (int i = 0; i < vocab_size; ++i)
            {
                float val = std::exp(sorted_logits[i].first);
                probabilities[i] = val;
                sum_exp += val;
            }
            for (int i = 0; i < vocab_size; ++i)
            {
                probabilities[i] /= sum_exp;
            }

            // Find the cutoff index where cumulative probability exceeds p
            float cumulative_prob = 0.0f;
            int cutoff_index = vocab_size - 1;
            for (int i = 0; i < vocab_size; ++i)
            {
                cumulative_prob += probabilities[i];
                if (cumulative_prob > p)
                {
                    cutoff_index = i;
                    break;
                }
            }

            // Resize vectors to [0..cutoff_index]
            float new_sum = 0.0f;
            for (int i = 0; i <= cutoff_index; ++i)
            {
                new_sum += probabilities[i];
            }
            for (int i = 0; i <= cutoff_index; ++i)
            {
                probabilities[i] /= new_sum;
            }

            probabilities.resize(cutoff_index + 1);
            sorted_logits.resize(cutoff_index + 1);

            // Multinomial sampling
            std::random_device rd;
            std::mt19937 gen(rd());
            std::discrete_distribution<> dist(probabilities.begin(), probabilities.end());
            return sorted_logits[dist(gen)].second;
        }

        // ------------------------
        // Temperature + Top-K + Top-P Sampler
        // ------------------------
        static int TemperatureTopKTopPSampler(const TfLiteTensor *logits,
                                              float temperature, int k, float p)
        {
            int vocab_size = logits->dims->data[2];
            std::vector<std::pair<float, int>> sorted_logits;
            sorted_logits.reserve(vocab_size);

            // 1) Apply Temperature
            std::vector<float> scaled_logits(vocab_size);
            for (int i = 0; i < vocab_size; ++i)
            {
                scaled_logits[i] = logits->data.f[i] / temperature;
            }

            // 2) Softmax over scaled logits
            float max_logit = *std::max_element(scaled_logits.begin(), scaled_logits.end());
            float sum_exp = 0.0f;
            for (int i = 0; i < vocab_size; ++i)
            {
                scaled_logits[i] = std::exp(scaled_logits[i] - max_logit);
                sum_exp += scaled_logits[i];
            }
            for (int i = 0; i < vocab_size; ++i)
            {
                scaled_logits[i] /= sum_exp;
                // Keep index-value pairs for sorting
                sorted_logits.emplace_back(scaled_logits[i], i);
            }

            // 3) Sort descending by probability
            std::sort(sorted_logits.begin(), sorted_logits.end(),
                      std::greater<std::pair<float, int>>());

            // 4) Top-K filter
            int top_k = std::min(k, vocab_size);
            sorted_logits.resize(top_k);

            // 5) Top-P filter within top-k
            float cumulative_prob = 0.0f;
            int cutoff_index = top_k - 1;
            for (int i = 0; i < top_k; ++i)
            {
                cumulative_prob += sorted_logits[i].first;
                if (cumulative_prob > p)
                {
                    cutoff_index = i;
                    break;
                }
            }
            sorted_logits.resize(cutoff_index + 1);

            // 6) Renormalize final probabilities
            float new_sum = 0.0f;
            for (auto &pair : sorted_logits)
            {
                new_sum += pair.first;
            }

            std::vector<float> final_probs;
            final_probs.reserve(sorted_logits.size());
            for (auto &pair : sorted_logits)
            {
                final_probs.push_back(pair.first / new_sum);
            }

            // 7) Multinomial sampling
            std::random_device rd;
            std::mt19937 gen(rd());
            std::discrete_distribution<> dist(final_probs.begin(), final_probs.end());
            return sorted_logits[dist(gen)].second;
        }
    };

    // --------------------------------------------------------------------------
    // Utility for applying XNNPACK weight caching
    // --------------------------------------------------------------------------
    void ApplyXNNPACKWeightCaching(tflite::Interpreter *interpreter)
    {
        auto delegate_options = TfLiteXNNPackDelegateOptionsDefault();
        std::string weight_cache_path = absl::GetFlag(FLAGS_weight_cache_path);
        // delegate_options.weight_cache_file_path = weight_cache_path.c_str();
        // delegate_options.num_threads = absl::GetFlag(FLAGS_num_threads);
        // delegate_options.flags |= TFLITE_XNNPACK_DELEGATE_FLAG_ENABLE_SUBGRAPH_RESHAPING;
        // delegate_options.flags |= TFLITE_XNNPACK_DELEGATE_FLAG_ENABLE_LATEST_OPERATORS;

        MINIMAL_CHECK(interpreter->ModifyGraphWithDelegate(
                          tflite::Interpreter::TfLiteDelegatePtr(
                              TfLiteXNNPackDelegateCreate(&delegate_options),
                              [](TfLiteDelegate *delegate)
                              { TfLiteXNNPackDelegateDelete(delegate); })) == kTfLiteOk);
    }

    // --------------------------------------------------------------------------
    // Loads the TFLite model
    // --------------------------------------------------------------------------
    std::unique_ptr<tflite::FlatBufferModel> LoadModel()
    {
        std::unique_ptr<tflite::FlatBufferModel> model =
            tflite::FlatBufferModel::BuildFromFile(absl::GetFlag(FLAGS_tflite_model).c_str());
        MINIMAL_CHECK(model != nullptr);
        return model;
    }

    // --------------------------------------------------------------------------
    // Builds a TFLite interpreter from the model and applies XNNPACK if requested
    // --------------------------------------------------------------------------
    std::unique_ptr<tflite::Interpreter>
    BuildInterpreter(tflite::FlatBufferModel *model, int num_threads)
    {
        tflite::ops::builtin::BuiltinOpResolver resolver;
        // Register GenAI custom ops
        tflite::ops::custom::GenAIOpsRegisterer(&resolver);

        tflite::InterpreterBuilder builder(*model, resolver);
        MINIMAL_CHECK(builder.SetNumThreads(num_threads) == kTfLiteOk);

        std::unique_ptr<tflite::Interpreter> interpreter;
        builder(&interpreter);
        MINIMAL_CHECK(interpreter != nullptr);

        if (!absl::GetFlag(FLAGS_weight_cache_path).empty())
        {
            ApplyXNNPACKWeightCaching(interpreter.get());
        }
        return interpreter;
    }

    // --------------------------------------------------------------------------
    // Constructs KV cache input structures for decode, based on the decode signature
    // --------------------------------------------------------------------------
    std::map<std::string, std::vector<float, AlignedAllocator<float>>>
    BuildKVCache(tflite::Interpreter *interpreter)
    {
        tflite::SignatureRunner *runner = interpreter->GetSignatureRunner("decode");
        if (runner == nullptr)
        {
            return {};
        }

        // Expect runner->input_size() = tokens, input_pos, plus 2*(num_layers)
        size_t num_layers = (runner->input_size() - 2) / 2;
        std::cout << "Num KV Cache Layers: " << num_layers << std::endl;
        if (num_layers == 0)
        {
            return {};
        }

        std::map<std::string, std::vector<float, AlignedAllocator<float>>> kv_cache;
        for (int i = 0; i < num_layers; ++i)
        {
            std::string k_cache_name = "kv_cache_k_" + std::to_string(i);
            std::string v_cache_name = "kv_cache_v_" + std::to_string(i);

            TfLiteTensor *tensor = runner->input_tensor(k_cache_name.c_str());
            size_t count = tensor->bytes / sizeof(float);

            kv_cache.emplace(k_cache_name,
                             std::vector<float, AlignedAllocator<float>>(count, 0.0f));
            kv_cache.emplace(v_cache_name,
                             std::vector<float, AlignedAllocator<float>>(count, 0.0f));
        }
        return kv_cache;
    }

    // --------------------------------------------------------------------------
    // Sets custom memory allocations for the KV cache on the given runner
    // --------------------------------------------------------------------------
    void PrepareRunner(tflite::SignatureRunner *runner,
                       std::map<std::string, std::vector<float, AlignedAllocator<float>>> &kv_cache)
    {
        for (auto &[name, cache] : kv_cache)
        {
            TfLiteCustomAllocation allocation{
                .data = static_cast<void *>(cache.data()),
                .bytes = cache.size() * sizeof(float)};

            MINIMAL_CHECK(runner->SetCustomAllocationForInputTensor(name.c_str(), allocation) == kTfLiteOk);
            MINIMAL_CHECK(runner->SetCustomAllocationForOutputTensor(name.c_str(), allocation) == kTfLiteOk);
        }
        MINIMAL_CHECK(runner->AllocateTensors() == kTfLiteOk);
    }

    // --------------------------------------------------------------------------
    // Finds the appropriate "prefill" runner for the given number of tokens.
    // If LoRA is used, it defers to LoRA's specialized runner selection.
    // --------------------------------------------------------------------------
    tflite::SignatureRunner *GetPrefillRunner(
        tflite::Interpreter *interpreter,
        std::size_t num_input_tokens,
        std::map<std::string, std::vector<float, AlignedAllocator<float>>> &kv_cache,
        const ai_edge_torch::examples::LoRA *lora)
    {
        tflite::SignatureRunner *runner = nullptr;
        int best_seq_size = -1;
        int delta = std::numeric_limits<int>::max();

        for (const std::string *key : interpreter->signature_keys())
        {
            if (!absl::StrContains(*key, "prefill") || absl::StrContains(*key, "lora"))
            {
                continue;
            }
            TfLiteTensor *input_pos =
                interpreter->GetSignatureRunner(key->c_str())->input_tensor("input_pos");
            int seq_size = input_pos->dims->data[0];

            // Choose the runner where seq_size >= num_input_tokens and
            // (seq_size - num_input_tokens) is minimized
            if (num_input_tokens <= static_cast<size_t>(seq_size) &&
                seq_size - static_cast<int>(num_input_tokens) < delta)
            {
                if (lora == nullptr)
                {
                    runner = interpreter->GetSignatureRunner(key->c_str());
                }
                best_seq_size = seq_size;
                delta = seq_size - static_cast<int>(num_input_tokens);
            }
        }

        // If LoRA is enabled, use the LoRA-specific prefill runner
        if (lora != nullptr)
        {
            runner = lora->GetPrefillRunner(interpreter, best_seq_size);
        }
        MINIMAL_CHECK(runner != nullptr);

        // Prepare KV memory allocations
        PrepareRunner(runner, kv_cache);
        return runner;
    }

    // --------------------------------------------------------------------------
    // Retrieves the decode runner (LoRA-based if needed) and prepares it
    // --------------------------------------------------------------------------
    tflite::SignatureRunner *GetDecodeRunner(
        tflite::Interpreter *interpreter,
        std::map<std::string, std::vector<float, AlignedAllocator<float>>> &kv_cache,
        ai_edge_torch::examples::LoRA *lora)
    {
        tflite::SignatureRunner *runner =
            (lora == nullptr)
                ? interpreter->GetSignatureRunner("decode")
                : lora->GetDecodeRunner(interpreter);
        MINIMAL_CHECK(runner != nullptr);

        PrepareRunner(runner, kv_cache);
        return runner;
    }

    // --------------------------------------------------------------------------
    // Loads the SentencePiece model from file
    // --------------------------------------------------------------------------
    std::unique_ptr<sentencepiece::SentencePieceProcessor> LoadSentencePieceProcessor()
    {
        std::ifstream input(absl::GetFlag(FLAGS_sentencepiece_model), std::ios::binary);
        std::string serialized_proto((std::istreambuf_iterator<char>(input)),
                                     std::istreambuf_iterator<char>());

        auto processor = std::make_unique<sentencepiece::SentencePieceProcessor>();
        MINIMAL_CHECK(processor->LoadFromSerializedProto(serialized_proto).ok());
        return processor;
    }


    void AnalyzeArenaAllocation(const tflite::Subgraph& subgraph) {
        std::cout << "\n=== Total Allocation Analysis ===\n";
        
        // Get the memory planner debug info
        // This shows the actual arena allocation plan
        subgraph.DumpMemoryPlannerDebugInfo();
        
        // Track arena types separately
        size_t total_arena_rw = 0;
        size_t total_arena_persistent = 0;
        size_t total_custom = 0;
        size_t total_mmap = 0;
        size_t total_dynamic = 0;
        size_t total_unknown = 0;
        
        // Analyze tensors by their arena allocation type
        for (size_t tensor_idx = 0; tensor_idx < subgraph.tensors_size(); tensor_idx++) {
            const TfLiteTensor* tensor = subgraph.tensor(tensor_idx);
            if (!tensor) continue;
            
            switch (tensor->allocation_type) {
                case kTfLiteArenaRw:
                    total_arena_rw += tensor->bytes;
                    std::cout << "Arena RW Tensor " << tensor_idx 
                             << ": " << tensor->bytes << " bytes\n";
                    break;
                case kTfLiteArenaRwPersistent:
                    total_arena_persistent += tensor->bytes;
                    std::cout << "Arena Persistent Tensor " << tensor_idx 
                             << ": " << tensor->bytes << " bytes\n";
                    break;
                case kTfLiteMmapRo:
                    total_mmap += tensor->bytes;
                    std::cout << "Memory-mapped Tensor " << tensor_idx 
                             << ": " << tensor->bytes << " bytes\n";
                    break;
                case kTfLiteDynamic:
                    total_dynamic += tensor->bytes;
                    std::cout << "Dynamic Tensor " << tensor_idx 
                             << ": " << tensor->bytes << " bytes\n";
                    break;
                case kTfLiteCustom:
                    total_custom += tensor->bytes;
                    break;
                default:
                    total_unknown += tensor->bytes;
                    break;
            }
        }
        
        std::cout << "\nArena Memory Summary:\n";
        std::cout << "--------------------\n";
        std::cout << "RW Arena: " << total_arena_rw << " bytes ("
                  << total_arena_rw / (1024.0 * 1024.0) << " MB)\n";
        std::cout << "Persistent Arena: " << total_arena_persistent << " bytes ("
                  << total_arena_persistent / (1024.0 * 1024.0) << " MB)\n";
        std::cout << "Memory-mapped: " << total_mmap << " bytes ("
                  << total_mmap / (1024.0 * 1024.0) << " MB)\n";
        std::cout << "Dynamic: " << total_dynamic << " bytes ("
                  << total_dynamic / (1024.0 * 1024.0) << " MB)\n";
        std::cout << "Custom: " << total_custom << " bytes ("
                  << total_custom / (1024.0 * 1024.0) << " MB)\n";
        std::cout << "Unknown: " << total_unknown << " bytes ("
                  << total_unknown / (1024.0 * 1024.0) << " MB)\n";
        std::cout << "Total: " << (total_arena_rw + total_arena_persistent + total_mmap + total_dynamic + total_custom + total_unknown)
                  << " bytes ("
                  << (total_arena_rw + total_arena_persistent + total_mmap + total_dynamic + total_custom + total_unknown) / (1024.0 * 1024.0)
                  << " MB)\n";
    }

    void AnalyzeDelegateExecution(tflite::Interpreter* interpreter) {
        // Lambda function to print tensor details
        auto print_tensor_details = [](int tensor_idx, TfLiteTensor* tensor) {
            if (!tensor) {
                std::cout << "Tensor " << tensor_idx << " is NULL\n";
                return;
            }
            
            // std::cout << "Tensor: " << tensor_idx << " ";
            
            void* tensor_data_address = tensor->data.raw;
            std::cout << "Data Address: " << tensor_data_address << " ";

            // Tensor Type
            const char* type_name = TfLiteTypeGetName(tensor->type);
            std::cout << "Type: " << (type_name ? type_name : "Unknown") << " ";

            // Tensor Allocation Type
            std::cout << "Allocation Type: ";
            switch (tensor->allocation_type) {
                case kTfLiteArenaRw:
                    std::cout << "Arena RW " << "Bytes: " << tensor->bytes << " ";
                    break;
                case kTfLiteArenaRwPersistent:
                    std::cout << "Arena Persistent " << "Bytes: " << tensor->bytes << " ";
                    break;
                case kTfLiteMmapRo:
                    std::cout << "Mmap " << "Bytes: " << tensor->bytes << " ";
                    break;
                case kTfLiteDynamic:
                    std::cout << "Dynamic " << "Bytes: " << tensor->bytes << " ";
                    break;
                case kTfLiteCustom:
                    std::cout << "Custom " << "Bytes: " << tensor->bytes << " ";
                    break;
                default:
                    std::cout << "Unknown " << "Bytes: 0 ";
                    break;
            }

            // Tensor Shape
            std::cout << "Shape: [";
            if(tensor->dims && tensor->dims->size > 0){
                for (int dim_idx = 0; dim_idx < tensor->dims->size; ++dim_idx) {
                    std::cout << tensor->dims->data[dim_idx];
                    if (dim_idx < tensor->dims->size - 1) std::cout << ", ";
                }
            }
            std::cout << "]\n";
        };
        
        std::cout << "\n=== Delegate Execution Analysis ===\n";
        std::cout << "===================================\n";
    
        // Analyze the primary subgraph's execution plan
        const tflite::Subgraph& primary_subgraph = interpreter->primary_subgraph();
        // std::cout <<  "Subgraph Name: " << primary_subgraph.name_ << std::endl;
        const std::vector<int>& execution_plan = primary_subgraph.execution_plan();
    
        std::vector<int> delegated_nodes;
        std::vector<int> non_delegated_nodes;
    
        for (int node_idx : execution_plan) {
            const auto* node_and_reg = primary_subgraph.node_and_registration(node_idx);
            const TfLiteNode* node = &node_and_reg->first;
            
            if (node->delegate) {
                delegated_nodes.push_back(node_idx);
            } else {
                non_delegated_nodes.push_back(node_idx);
            }
        }
    
        //std::cout << "Delegate Analysis:\n";
        std::cout << "  Total Nodes: " << execution_plan.size() << "\n";
        std::cout << "  Delegated Nodes: " << delegated_nodes.size() << "\n";
        std::cout << "  Non-Delegated Nodes: " << non_delegated_nodes.size() << "\n";
        int num_input_tensors = 0;
        int num_output_tensors = 0;
        int num_intermediate_tensors = 0;
        int num_temporary_tensors = 0;
        int num_total_tensors = 0;

        // Detailed node analysis based on execution order
        std::cout << "=== Node Details ===" << std::endl;
        for(int node_idx: execution_plan) {
            const auto* node_and_reg = primary_subgraph.node_and_registration(node_idx);
            const TfLiteNode* node = &node_and_reg->first;
            const TfLiteRegistration& registration = node_and_reg->second;

            std::cout << "  Node " << node_idx << ":\n";
            std::cout << "    Operator: " 
                      << (registration.builtin_code ? 
                          tflite::EnumNameBuiltinOperator(static_cast<tflite::BuiltinOperator>(registration.builtin_code)) 
                          : "Custom Operator") 
                      << "\n";
            // Print Input Tensor Indices
            std::cout << "    Input Tensors:\n";
            for (int i = 0; i < node->inputs->size; ++i) {
                //  allocation type address      allocate   
                
                std::cout << "      Input " << i << ": " << node->inputs->data[i] << " ";
                
                uint32_t tensor_idx = node->inputs->data[i];
                auto* tensor = interpreter->tensor(tensor_idx);
                print_tensor_details(tensor_idx, tensor);
                
                ++num_input_tensors;
            }
            
            // Print Output Tensor Indices
            std::cout << "    Output Tensors:\n";
            for (int i = 0; i < node->outputs->size; ++i) {
                std::cout << "      Output " << i << ": " << node->outputs->data[i] << " ";

                uint32_t tensor_idx = node->outputs->data[i];
                auto* tensor = interpreter->tensor(tensor_idx);
                print_tensor_details(tensor_idx, tensor);

                ++num_output_tensors;
            }
            
            // Print Intermediate Tensor Indices
            std::cout << "    Intermediate Tensors:\n";
            for (int i = 0; i < node->intermediates->size; ++i) {
                std::cout << "      Intermediate " << i << ": " << node->intermediates->data[i] << " ";

                uint32_t tensor_idx = node->intermediates->data[i];
                auto* tensor = interpreter->tensor(tensor_idx);
                print_tensor_details(tensor_idx, tensor);

                ++num_intermediate_tensors;
            }
            
            // Print Temporary Tensor Indices
            std::cout << "    Temporary Tensors:\n";
            for (int i = 0; i < node->temporaries->size; ++i) {
                std::cout << "      Temporary " << i << ": " << node->temporaries->data[i] << " ";

                uint32_t tensor_idx = node->temporaries->data[i];
                auto* tensor = interpreter->tensor(tensor_idx);
                print_tensor_details(tensor_idx, tensor);

                ++num_temporary_tensors;
            }
        }

        //  count   .
        std::cout << "==== Total Tensors ====\n";
        std::cout << "Total Tensors: " << num_input_tensors + num_output_tensors + num_intermediate_tensors + num_temporary_tensors 
                    << "\n Input Tensors: " << num_input_tensors
                    << "\n Output Tensors: " << num_output_tensors
                    << "\n Intermediate Tensors: " << num_intermediate_tensors
                    << "\n Temporary Tensors: " << num_temporary_tensors << std::endl;
    }

    class KVCacheManager {
        public:
            using KVCache = std::map<std::string, std::vector<float, AlignedAllocator<float>>>;
        
            static bool LockCache(KVCache& cache) {
                for (auto& [name, vec] : cache) {
                    if (!LockVector(vec)) {
                        // If locking fails, unlock everything we've locked so far
                        UnlockCache(cache);
                        return false;
                    }
                }
                return true;
            }
        
            static void UnlockCache(KVCache& cache) {
                for (auto& [name, vec] : cache) {
                    UnlockVector(vec);
                }
            }
        
        private:
            static bool LockVector(std::vector<float, AlignedAllocator<float>>& vec) {
                if (vec.empty()) {
                    return true;
                }
        
                // Get the pointer to the vector's data
                float* data = vec.data();
                
                // Calculate the actual size including padding
                size_t rawSize = vec.size() * sizeof(float);
                size_t padding = tflite::kDefaultTensorAlignment - 
                                (rawSize % tflite::kDefaultTensorAlignment);
                size_t totalSize = rawSize + padding;
        
                // Lock the memory - we know it's already properly aligned thanks to AlignedAllocator
                if (mlock(data, totalSize) != 0) {
                    // std::cerr << "Failed to lock memory for vector: " 
                    //           << strerror(errno) << std::endl;
                    return false;
                }
        
                // Optionally prevent the memory from being inherited by child processes
                // if (madvise(data, totalSize, MADV_DONTFORK) != 0) {
                //     std::cerr << "Warning: madvise MADV_DONTFORK failed: "
                //               << strerror(errno) << std::endl;
                //     // Continue anyway as this is not critical
                // }
        
                return true;
            }
        
            static void UnlockVector(std::vector<float, AlignedAllocator<float>>& vec) {
                if (vec.empty()) {
                    return;
                }
        
                float* data = vec.data();
                size_t rawSize = vec.size() * sizeof(float);
                size_t padding = tflite::kDefaultTensorAlignment - 
                                (rawSize % tflite::kDefaultTensorAlignment);
                size_t totalSize = rawSize + padding;
        
                if (munlock(data, totalSize) != 0) {
                    std::cerr << "Warning: Failed to unlock memory: "
                              << strerror(errno) << std::endl;
                }
            }
        };
        

} // end anonymous namespace

// =======================================================================
// main() entry
// =======================================================================
int main(int argc, char *argv[])
{
    // 0. Parse flags
    absl::ParseCommandLine(argc, argv);
    std::cout << "[INFO] Preparing Required Components\n";

    // Global variables
    std::unique_ptr<tflite::FlatBufferModel> model;
    std::unique_ptr<tflite::Interpreter> interpreter;
    std::unique_ptr<sentencepiece::SentencePieceProcessor> sp_processor;
    std::map<std::string, std::vector<float, AlignedAllocator<float>>> kv_cache;
    std::unique_ptr<ai_edge_torch::examples::LoRA> lora = nullptr;
    std::vector<int> prompt_tokens;
    std::string prompt, start_token, stop_token;
    int stop_token_id = -1;

    // 1. Load Model
    {
        ScopeTimer timer("Model Loading");
        model = LoadModel();
    }

    std::unique_ptr<uint8_t[]> combined_buffer;
    // 2. Build Interpreter
    {
        ScopeTimer timer("Interpreter Building");
        interpreter = BuildInterpreter(model.get(), absl::GetFlag(FLAGS_num_threads));

        // reorderTensorV3(interpreter);
        // reorderTensorV4(interpreter);
        // reorderTensorV5(interpreter);
        // reorderTensor2(interpreter);
    }

    // #ifdef TF_LITE_TENSORFLOW_PROFILER
    //      auto profiler = std::make_unique<tflite::profiling::Profiler>();
    //     //std::make_unique<tflite::profiling::Profiler> profiler;
    //     interpreter->SetProfiler(std::move(profiler));
    //     //interpreter->SetProfiler();
    // #endif

    // 3. Load SentencePiece
    {
        ScopeTimer timer("SentencePiece Loading");
        sp_processor = LoadSentencePieceProcessor();
    }

    // 4. Build KV Cache
    {
        ScopeTimer timer("KV Cache Building");
        kv_cache = BuildKVCache(interpreter.get());
        // if (!KVCacheManager::LockCache(kv_cache)) {
        //     std::cerr << "Failed to lock KV cache in memory" << std::endl;
        //     return 1;
        // }   
        MINIMAL_CHECK(!kv_cache.empty());
    }

    // 5. Optionally load LoRA
    {
        ScopeTimer timer("LoRA Loading");
        if (!absl::GetFlag(FLAGS_lora_path).empty())
        {
            lora = ai_edge_torch::examples::LoRA::FromFile(absl::GetFlag(FLAGS_lora_path));
            MINIMAL_CHECK(lora != nullptr);
        }
    }

    // 6. Prepare Input Prompt
    {
        ScopeTimer timer("Input Prompt Preparation");
        prompt = absl::GetFlag(FLAGS_prompt);
        MINIMAL_CHECK(sp_processor->Encode(prompt, &prompt_tokens).ok());

        start_token = absl::GetFlag(FLAGS_start_token);
        if (!start_token.empty())
        {
            prompt_tokens.insert(prompt_tokens.begin(), sp_processor->PieceToId(start_token));
        }

        stop_token = absl::GetFlag(FLAGS_stop_token);
        if (!stop_token.empty())
        {
            stop_token_id = sp_processor->PieceToId(stop_token);
        }
    }

    // 7. Prepare Signature Runners
    tflite::SignatureRunner *prefill_runner = nullptr;
    tflite::SignatureRunner *decode_runner = nullptr;
    {
        ScopeTimer timer("Signature Runners Preparation");
        std::size_t effective_prefill_token_size =
            (prompt_tokens.size() > 0) ? (prompt_tokens.size() - 1) : 0;

        prefill_runner = GetPrefillRunner(
            interpreter.get(), effective_prefill_token_size, kv_cache, lora.get());
        MINIMAL_CHECK(prefill_runner != nullptr);

        decode_runner = GetDecodeRunner(interpreter.get(), kv_cache, lora.get());
        MINIMAL_CHECK(decode_runner != nullptr);

        // AnalyzeArenaAllocation(interpreter->primary_subgraph());
        AnalyzeDelegateExecution(interpreter.get());
    }

    

    // 8. Access Tensors
    TfLiteTensor *prefill_input = prefill_runner->input_tensor("tokens");
    TfLiteTensor *prefill_input_pos = prefill_runner->input_tensor("input_pos");
    TfLiteTensor *decode_input = decode_runner->input_tensor("tokens");
    TfLiteTensor *decode_input_pos = decode_runner->input_tensor("input_pos");
    TfLiteTensor *kv_cache_k_0 = decode_runner->input_tensor("kv_cache_k_0");

    int max_seq_size = prefill_input->dims->data[1];
    int kv_cache_max_size = kv_cache_k_0->dims->data[1];

    // 9. Prefill Stage
    {
        ScopeTimer timer("Prefill Stage");
        int prefill_seq_size = std::min<int>(prompt_tokens.size(), max_seq_size);

        // Zero out the input tensors
        std::memset(prefill_input->data.i32, 0, prefill_input->bytes);
        std::memset(prefill_input_pos->data.i32, 0, prefill_input_pos->bytes);

        // Prefill uses all but the last token from the prompt
        for (int i = 0; i < prefill_seq_size - 1; ++i)
        {
            prefill_input->data.i32[i] = prompt_tokens[i];
            prefill_input_pos->data.i32[i] = i;
        }

        // Execute the prefill runner
        MINIMAL_CHECK(prefill_runner->Invoke() == kTfLiteOk);
    }

    // 10. Decoding Stage with separate metrics for inference and sampling
    std::cout << "\nPrompt:\n"
              << prompt << "\n\nOutput Text:\n";

    // Metrics object
    DecodingMetrics decoding_metrics;
    decoding_metrics.StartDecoding();

    {
        // ScopeTimer timer("Decoding Stage");

        // Determine how many tokens to generate
        int max_decode_steps = (absl::GetFlag(FLAGS_max_decode_steps) == -1)
                                   ? kv_cache_max_size
                                   : absl::GetFlag(FLAGS_max_decode_steps);

        int prefill_seq_size = std::min<int>(prompt_tokens.size(), max_seq_size);
        int decode_steps = std::min<int>(max_decode_steps, kv_cache_max_size - prefill_seq_size);
        MINIMAL_CHECK(decode_steps > 0);

        int next_token = prompt_tokens[prefill_seq_size - 1];
        int next_position = prefill_seq_size - 1;

        // Decoding loop
        for (int i = 0; i < decode_steps; ++i)
        {
            // Start time for this token
            auto token_start = std::chrono::high_resolution_clock::now();

            // -----------------------
            // 1) Model Inference
            // -----------------------
            auto inference_start = std::chrono::high_resolution_clock::now();
            // kv cache      decode  ?
            // Override   

            decode_input->data.i32[0] = next_token;
            decode_input_pos->data.i32[0] = next_position;
            MINIMAL_CHECK(decode_runner->Invoke() == kTfLiteOk);

            auto inference_end = std::chrono::high_resolution_clock::now();
            double inference_time_ms =
                std::chrono::duration<double, std::milli>(inference_end - inference_start).count();

            // -----------------------
            // 2) Token Sampling
            // -----------------------
            auto sampling_start = std::chrono::high_resolution_clock::now();
            next_token = Sampler::TemperatureTopKTopPSampler(
                decode_runner->output_tensor("logits"), 0.9f, 85, 0.9f);
            auto sampling_end = std::chrono::high_resolution_clock::now();
            double sampling_time_ms =
                std::chrono::duration<double, std::milli>(sampling_end - sampling_start).count();

            next_position++;

            // Check stop token
            if (next_token == stop_token_id)
            {
                break;
            }

            // Decode the single token to text
            std::vector<int> single_token_vec = {next_token};
            std::string single_decoded_text;
            MINIMAL_CHECK(sp_processor->Decode(single_token_vec, &single_decoded_text).ok());
            std::cout << single_decoded_text << std::flush;

            // Record metrics for this token
            decoding_metrics.RecordTimes(token_start, inference_time_ms, sampling_time_ms);
        }
    }

    // 11. Print decoding metrics (inference vs. sampling)
    decoding_metrics.PrintMetrics();

    // #ifdef TF_LITE_TENSORFLOW_PROFILER
    // // After running your model:
    // std::cout << "Compiled with Profiler" << std::endl;
    // if (profiler) {
    //     const auto& profile_events = profiler->GetProfileEvents();
    //     std::cout << "=== Profile events ===" << std::endl;
    //     for (const auto& event : profile_events) {
    //         std::cout << "Event: " << event->tag 
    //                 << " Begin: " << event->begin_timestamp_us 
    //                 << " Elapsed time (us): " << (event->elapsed_time)
    //                 << std::endl;
    //     }
    // }
    // #endif
    // KVCacheManager::UnlockCache(kv_cache);

    return 0;
}
